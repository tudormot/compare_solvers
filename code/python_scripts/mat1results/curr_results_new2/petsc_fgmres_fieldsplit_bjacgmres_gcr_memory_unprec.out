/var/slurmd-mpp2/job635347/slurm_script: line 16: ./load_modules_mpp2cluster: Permission denied
warning:seems as no solution array provided in input file(or err in read nsz vector)
Timing of PETSc INIT1 0.0162811
WARNING: using a PETSc matrix storage for non-symmetric matrices on a symmetric matrix. Wasting space and solving possibly slower
Timing of PETSc INIT2 6.41585
KSP Object: 1 MPI processes
  type: gcr
    restart = 30 
    restarts performed = 0 
  maximum iterations=2000, initial guess is zero
  tolerances:  relative=1e-08, absolute=1e-50, divergence=10000.
  right preconditioning
  using UNPRECONDITIONED norm type for convergence test
PC Object: 1 MPI processes
  type: fieldsplit
    FieldSplit with Schur preconditioner, blocksize = 1, factorization UPPER
    Preconditioner for the Schur complement formed from S itself
    Split info:
    Split number 0 Defined by IS
    Split number 1 Defined by IS
    KSP solver for A00 block
  KSP Object: (fieldsplit_0_) 1 MPI processes
    type: gmres
      restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
      happy breakdown tolerance 1e-30
    maximum iterations=10000, initial guess is zero
    tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
    left preconditioning
    using DEFAULT norm type for convergence test
  PC Object: (fieldsplit_0_) 1 MPI processes
    type: bjacobi
    PC has not been set up so information may be incomplete
      number of blocks = -1
      Local solve is same for all blocks, in the following KSP and PC objects:
    linear system matrix = precond matrix:
    Mat Object: (fieldsplit_0_) 1 MPI processes
      type: seqaij
      rows=1260320, cols=1260320
      total: nonzeros=133307458, allocated nonzeros=133307458
      total number of mallocs used during MatSetValues calls =0
        using I-node routines: found 420038 nodes, limit used is 5
    KSP solver for S = A11 - A10 inv(A00) A01 
  KSP Object: (fieldsplit_1_) 1 MPI processes
    type: gmres
      restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
      happy breakdown tolerance 1e-30
    maximum iterations=10000, initial guess is zero
    tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
    left preconditioning
    using DEFAULT norm type for convergence test
  PC Object: (fieldsplit_1_) 1 MPI processes
    type: bjacobi
    PC has not been set up so information may be incomplete
      number of blocks = -1
      Local solve is same for all blocks, in the following KSP and PC objects:
    linear system matrix = precond matrix:
    Mat Object: (fieldsplit_1_) 1 MPI processes
      type: schurcomplement
      rows=0, cols=0
        Schur complement A11 - A10 inv(A00) A01
        A11
          Mat Object: (fieldsplit_1_) 1 MPI processes
            type: seqaij
            rows=0, cols=0
            total: nonzeros=0, allocated nonzeros=0
            total number of mallocs used during MatSetValues calls =0
              not using I-node routines
        A10
          Mat Object: 1 MPI processes
            type: seqaij
            rows=0, cols=1260320
            total: nonzeros=0, allocated nonzeros=0
            total number of mallocs used during MatSetValues calls =0
              not using I-node routines
        KSP of A00
  KSP Object: (fieldsplit_0_) 1 MPI processes
    type: gmres
      restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
      happy breakdown tolerance 1e-30
    maximum iterations=10000, initial guess is zero
    tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
    left preconditioning
    using DEFAULT norm type for convergence test
  PC Object: (fieldsplit_0_) 1 MPI processes
    type: bjacobi
    PC has not been set up so information may be incomplete
      number of blocks = -1
      Local solve is same for all blocks, in the following KSP and PC objects:
    linear system matrix = precond matrix:
    Mat Object: (fieldsplit_0_) 1 MPI processes
      type: seqaij
      rows=1260320, cols=1260320
      total: nonzeros=133307458, allocated nonzeros=133307458
      total number of mallocs used during MatSetValues calls =0
        using I-node routines: found 420038 nodes, limit used is 5
        A01
          Mat Object: 1 MPI processes
            type: seqaij
            rows=1260320, cols=0
            total: nonzeros=0, allocated nonzeros=0
            total number of mallocs used during MatSetValues calls =0
              using I-node routines: found 252064 nodes, limit used is 5
  linear system matrix = precond matrix:
  Mat Object: 1 MPI processes
    type: seqaij
    rows=1260320, cols=1260320
    total: nonzeros=133307458, allocated nonzeros=133307458
    total number of mallocs used during MatSetValues calls =0
      using I-node routines: found 420038 nodes, limit used is 5
  0 KSP unpreconditioned resid norm 2.414708141649e+06 true resid norm 2.414708141649e+06 ||r(i)||/||b|| 1.000000000000e+00
  1 KSP unpreconditioned resid norm 9.529732862462e+03 true resid norm 9.529732862462e+03 ||r(i)||/||b|| 3.946536104340e-03
slurmstepd: *** STEP 635347.0 ON mpp2r04c03s01 CANCELLED AT 2018-07-21T13:44:33 DUE TO TIME LIMIT ***
slurmstepd: *** JOB 635347 ON mpp2r04c03s01 CANCELLED AT 2018-07-21T13:44:33 DUE TO TIME LIMIT ***
[0]PETSC ERROR: ------------------------------------------------------------------------
[0]PETSC ERROR: Caught signal number 15 Terminate: Some process (or the batch system) has told this process to end
[0]PETSC ERROR: Try option -start_in_debugger or -on_error_attach_debugger
[0]PETSC ERROR: or see http://www.mcs.anl.gov/petsc/documentation/faq.html#valgrind
[0]PETSC ERROR: or try http://valgrind.org on GNU/linux and Apple Mac OS X to find memory corruption errors
[0]PETSC ERROR: configure using --with-debugging=yes, recompile, link, and run 
[0]PETSC ERROR: to get more information on the crash.
[0]PETSC ERROR: --------------------- Error Message --------------------------------------------------------------
[0]PETSC ERROR: Signal received
[0]PETSC ERROR: See http://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.
[0]PETSC ERROR: Petsc Release Version 3.9.1, Apr, 29, 2018 
[0]PETSC ERROR: /home/hpc/pr63so/ga53lov2/timing_results_petsc.txt on a  named mpp2r04c03s01 by ga53lov2 Sat Jul 21 09:45:11 2018
[0]PETSC ERROR: Configure options --with-clean=0 --prefix=/lrz/sys/libraries/petsc/3.9.1/Medium_Node/arch-linux-3-c-3.9.1-Medium_Node-real_mpi.intel_5.1_opt_gene --with-debugging=0 --with-valgrind-dir=/lrz/sys/tools/valgrind/3.10.0 --with-scalar-type=real --with-blaslapack-dir=/lrz/sys/intel/studio2017_u6/compilers_and_libraries_2017.6.256/linux/mkl PETSC_DIR=/lrz/noarch/src/petsc/petsc-3.9.1 PETSC_ARCH=arch-linux-3-c-3.9.1-Medium_Node-real_mpi.intel_5.1_opt_gene COPTFLAGS="-O3 -xHOST -pipe" CXXOPTFLAGS="-O3 -xHOST -pipe" FOPTFLAGS="-O3 -xHOST -pipe" --with-scalapack=1 --with-scalapack-lib="-L/lrz/sys/intel/studio2017_u6/compilers_and_libraries_2017.6.256/linux/mkl/lib/intel64 -Wl,-static -lmkl_scalapack_lp64 -lmkl_blacs_intelmpi_lp64 -Wl,-dy" --known-level1-dcache-linesize=32 --known-level1-dcache-assoc=0 --known-memcmp-ok=1 --known-sizeof-char=1 --known-sizeof-void-p=8 --known-sizeof-short=2 --known-sizeof-int=4 --known-sizeof-long=8 --known-sizeof-long-long=8 --known-sizeof-float=4 --known-sizeof-double=8 --known-sizeof-size_t=8 --known-bits-per-byte=8 --known-sizeof-MPI_Comm=4 --known-sizeof-MPI_Fint=4 --known-mpi-long-double=1 --known-mpi-int64_t=1 --known-sdot-returns-double=0 --known-snrm2-returns-double=0 --known-has-attribute-aligned=1 --with-fortran --with-clanguage=cxx --known-mpi-shared-libraries=no --with-batch=0 --with-64-bit-pointers=1
[0]PETSC ERROR: #1 User provided function() line 0 in  unknown file
application called MPI_Abort(MPI_COMM_WORLD, 59) - process 0
